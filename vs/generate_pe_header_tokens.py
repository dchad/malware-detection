# generate-pe-header-tokens.py
#
# Parse a bunch of PE Header dump files generated by objdump and
# extract keywords such section names, import DLLs and functions.
# These tokens will be used for feature extraction from the
# PE/COFF headers.
#
# Inputs : list of PE Header files.
#          Temp file name for token counts.
#          File name for combined token counts.
#
# Outputs: pe-header-tokens.txt
#          pe-header-token-counts.csv
#          row format = [token_name, count]
#
# Author: Derek Chadwick
# Date  : 12/09/2016


from multiprocessing import Pool
import os
from csv import writer
import numpy as np
import pandas as pd
import re


# Determine if a string is all ascii characters.

def is_ascii(s):
    return all(ord(c) < 128 for c in s)


def is_printable_ascii(s):
    return all(ord(c) > 31 and ord(c) < 127 for c in s)


def save_token_counts(token_counter_map, out_file_name):
    # Output the PE Header token counts.
    pid = os.getpid()
    out_file = "data/" + str(pid) + "-" + out_file_name
    fop = open(out_file, 'w')
    csv_wouter = writer(fop)

    outlines = []
    sorted_keys = token_counter_map.keys()
    sorted_keys.sort()
    counter = 0
    
    for key in sorted_keys:
        outlines.append([key, token_counter_map[key]])
        counter += 1
        if (counter % 100) == 0: # write out some lines
            csv_wouter.writerows(outlines)
            outlines = []
            print("Processed token {:s} -> {:d}.".format(key, token_counter_map[key]))

    # Finish off.
    if (len(outlines) > 0):
        csv_wouter.writerows(outlines)
        outlines = []

    print("Completed writing {:d} tokens.".format(len(sorted_keys)))    
    fop.close()

    return


def get_token_count_map(token_df):
    # Read in the token count file and create a dict.
    token_dict = {}
    type_y = np.array(token_df['token_name'])
    
    for idx in range(token_df.shape[0]): # First fill the dict with the token counts
        token_dict[token_df.iloc[idx,0]] = token_df.iloc[idx,1]
        

    return token_dict

    
def generate_pe_tokens(mp_params):
    # Parse a bunch of PE/COFF headers dumped by objdump and extract
    # section names, import DLLs, import functions and exported functions.
    file_list = mp_params.file_list
    out_count_file = mp_params.count_file
    
    psections = re.compile('\s+\d{1,2}\s+(\.\w+|\w+)\s+\d+')  # Pattern for section names.
    pdlls = re.compile('\s+DLL Name: (\w+)')                  # Pattern for import DLL names.
    pfunctions = re.compile('\s+\w+\s+\d{1,4}\s+(.+)\s*')     # Pattern for import function names.
    preloc = re.compile('\s+reloc')                           # Pattern for relocation entries.
    pexports = re.compile('\s+\[\s*\d+\]\s+(\w+)')            # Pattern for exported function names.
    
    token_counter_map = {}
    counter = 0
    pid = os.getpid()
    
    for idx, fname in enumerate(file_list):

        fip = open(fname, 'r')
        in_lines = fip.readlines()
        
        counter += 1
        
        for line in in_lines:

            line = line.rstrip() # get rid of newlines they are annoying.
            token_val = ""


            
            m = preloc.match(line)
            if m != None:
                #token_val = m.group(2)
                continue

            m = psections.match(line)
            if m != None:
                token_val = m.group(1)
                #print("Section: {:s}".format(token_val))
            else:
                m = pdlls.match(line)
                if m != None:
                    token_val = m.group(1)
                else:
                    m = pfunctions.match(line)
                    if m != None:
                        token_val = m.group(1)
                    else:                 
                        m = pexports.match(line)
                        if m != None:
                            token_val = m.group(1)
                            #print("Export: {:s}".format(token_val))
                        else:   
                            continue
                        
            # Clean the token name, the function name regex is picking up random crap.
            idx = token_val.find('\t')
            if idx > 0:
                token_val = token_val[0:idx] # take the token up to the first tab.
                
            if not is_printable_ascii(token_val):
                continue

            # Now demangle C++ names, they are annoying.
            token_val = token_val.replace('@','').replace('$','').replace('?','') #.replace('\t','')
            
            # Count the token type.
            if token_val in token_counter_map.keys():
                token_counter_map[token_val] += 1
            else:
                token_counter_map[token_val] = 1


        if (counter % 1000) == 0:
            print("{:d} Processed {:d} header files.".format(pid, counter))

        fip.close()
        
        
    save_token_counts(token_counter_map, out_count_file)
    
    return


def save_combine(token_counter_map, out_file_name):
    # Save the combined token counts.
    
    out_file = "data/" + out_file_name
    fop = open(out_file, 'w')
    csv_wouter = writer(fop)
    cols = ['token_name','count'] 
    csv_wouter.writerow(cols)
    
    outlines = []
    sorted_keys = token_counter_map.keys()
    sorted_keys.sort()
    counter = 0
    
    for key in sorted_keys:
        outlines.append([key, token_counter_map[key]])
        counter += 1
        if (counter % 100) == 0: # write out some lines
            csv_wouter.writerows(outlines)
            outlines = []
            print("Processed token {:s} -> {:d}.".format(key, token_counter_map[key]))

    # Finish off.
    if (len(outlines) > 0):
        csv_wouter.writerows(outlines)
        outlines = []

    fop.close()
    
    print("Completed writing {:d} tokens.".format(len(sorted_keys)))  
    
    return


def combine_token_files(token_file, count_file):
    # Function to combine the newly generated token files into one file:
    # 1. list data directory
    # 2. For each file in file list that matches (\d\d\d\d-pe-header-tokens.csv)
    # 3. Trim the filenames if necessary (should remove VirusShare_  prefix).
    # 4. Concatenate the unsorted token feature files.
    # 5. Sort and write to data/sorted-token-features.csv

    
    p1 = re.compile('\d{3,5}-' + count_file) # This is the PID prefix for each file.
    file_list = os.listdir('data/')
    counter = 0
    token_map = {}
    
    for file_name in file_list:
        if p1.match(file_name):
            fip = open('data/' + file_name, 'r')
            in_lines = fip.readlines()
            for line in in_lines:
                tokens = line.split(',')
                if tokens[0] not in token_map.keys():
                    token_map[tokens[0]] = int(tokens[1])
                else:
                    token_map[tokens[0]] += int(tokens[1])
                    
            counter += len(in_lines)
            fip.close()
            
 

    save_combine(token_map, token_file)
    
    print('Completed combine of {:d} PE/COFF header tokens.'.format(counter)) 
    
    return


class Multi_Params(object):
    def __init__(self, tokenfile="", countfile="", filelist=[]):
        self.token_file = tokenfile
        self.count_file = countfile
        self.file_list = filelist
        

# Start of script.
if __name__ == "__main__":

    #TODO: parse command line options for input/output file names.

    #file_list = os.listdir('/opt/vs/train1hdr/')
    #generate_pe_tokens(file_list,'data/pe-header-tokens-vs251.txt','data/pe-header-token-counts-vs251.csv')

    token_file = 'pe-header-tokens-vs251.csv'
    count_file = 'pe-header-token-counts-vs251.csv'
    ext_drive = '/opt/vs/train1hdr/'

    #token_file = 'pe-header-tokens-vs252.csv'
    #count_file = 'pe-header-token-counts-vs252.csv'
    #ext_drive = '/opt/vs/train2hdr/'

    #token_file = 'pe-header-tokens-vs263.csv'
    #count_file = 'pe-header-token-counts-vs263.csv'
    #ext_drive = '/opt/vs/train3hdr/'

    #token_file = 'pe-header-tokens-vs264.csv'
    #count_file = 'pe-header-token-counts-vs264.csv'
    #ext_drive = '/opt/vs/train4hdr/'

    file_list = os.listdir(ext_drive)
    tfiles = []

    for fname in file_list:
        tfiles.append(ext_drive + fname)
        
    quart = len(tfiles)/4
    train1 = tfiles[:quart]
    train2 = tfiles[quart:(2*quart)]
    train3 = tfiles[(2*quart):(3*quart)]
    train4 = tfiles[(3*quart):]

    print("Generate PE/COFF Header Tokens: {:s}".format(token_file))
    print("Files: {:d} - {:d} - {:d}".format(len(tfiles), quart, (len(train1)+len(train2)+len(train3)+len(train4))))

    mp1 = Multi_Params(token_file, count_file, train1)
    mp2 = Multi_Params(token_file, count_file, train2)
    mp3 = Multi_Params(token_file, count_file, train3)
    mp4 = Multi_Params(token_file, count_file, train4)

    trains = [mp1, mp2, mp3, mp4]
    p = Pool(4)
    p.map(generate_pe_tokens, trains)

    combine_token_files(token_file, count_file)

# End of Script
