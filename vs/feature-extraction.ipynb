{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation and Extraction of Features\n",
    "    Features sets will consist of:\n",
    "    - Entropy and file size from packed binaries.\n",
    "    - Entropy and file size from unpacked binaries.\n",
    "    - ASM features from disassembled unpacked binaries.\n",
    "    - Call Graph Features.\n",
    "    - Sample Statistics.\n",
    "    - PE packer type.\n",
    "    - Behavioural features from Cuckoo Sandbox reports.\n",
    "    - Memory features from Volatility reports.\n",
    "    \n",
    "    Training labels will be generated from ClamAV and VirusTotal.com reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import os\n",
    "from csv import writer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import scipy.misc\n",
    "import array\n",
    "import time as tm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ext_drive = '/opt/vs/'\n",
    "tfiles = os.listdir(ext_drive + \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65536"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Generate Entropy and File Size of Packed Binaries and Non-Binary Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate Shannon's Entropy, https://en.wikipedia.org/wiki/Entropy_(information_theory)\n",
    "\n",
    "def calculate_entropy(byte_counts, total):\n",
    "    \n",
    "    entropy = 0.0\n",
    "\n",
    "    for count in byte_counts:\n",
    "        # If no bytes of this value were seen in the value, it doesn't affect\n",
    "        # the entropy of the file.\n",
    "        if count == 0:\n",
    "            continue\n",
    "        # p is the probability of seeing this byte in the file, as a floating-point number\n",
    "        p = 1.0 * count / total\n",
    "        entropy -= p * math.log(p, 256)\n",
    "    \n",
    "\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def entropy_counter(byte_code):\n",
    "    \n",
    "    byte_counts = [0] * 256\n",
    "    code_length = len(byte_code)\n",
    "    \n",
    "    for i in range(len(byte_code)):\n",
    "        byte_counts[int(byte_code[i])] += 1\n",
    "        \n",
    "    entropy = calculate_entropy(byte_counts, code_length)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def sort_and_save_entropy_feature_file():\n",
    "    entropys = pd.read_csv('data/entropy-features.csv')\n",
    "    # DataFrame.sort() is deprecated, but this is an old version of pandas, does not have sort_values().\n",
    "    sorted_entropys = entropys.sort('file_name')\n",
    "    sorted_entropys.to_csv('data/sorted-entropy-features.csv', index=False)\n",
    "    sorted_entropys.head(20)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def combine_entropy_files():\n",
    "    # Function to combine the newly generated entropy files into one file:\n",
    "    # 1. list data directory\n",
    "    # 2. For each file in file list that matches (\\d\\d\\d\\d-entropy-features.csv)\n",
    "    # 3. Trim the filenames if necessary (should remove VirusShare_  prefix).\n",
    "    # 4. Concatenate the unsorted packer id feature files.\n",
    "    # 5. Sort and write to data/sorted-packer-id-features.csv\n",
    "    fop = open('data/entropy-features.csv','w')\n",
    "    fop.write('file_name,entropy,file_size\\n')\n",
    "    p1 = re.compile('\\d{3,5}-entropy-features-bin.csv') # This is the PID prefix for each file.\n",
    "    file_list = os.listdir('data/')\n",
    "    counter = 0\n",
    "    for file_name in file_list:\n",
    "        if p1.match(file_name):\n",
    "            fip = open('data/' + file_name, 'r')\n",
    "            in_lines = fip.readlines()\n",
    "            fop.writelines(in_lines)\n",
    "            counter += len(in_lines)\n",
    "            fip.close()\n",
    "            \n",
    "    print('Completed combine of {:d} entropy features.'.format(counter))  \n",
    "    \n",
    "    fop.close()\n",
    "    \n",
    "    sort_and_save_entropy_feature_file()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "# feature extraction for the binary files\n",
    "\n",
    "def extract_binary_features(tfiles):\n",
    "    #byte_files = [i for i in tfiles if '.bytes' in i]\n",
    "    ftot = len(tfiles)\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    print('Process id:', pid)\n",
    "    feature_file = 'data/' + str(pid) + '-entropy-features-bin.csv' # entropy, file size, ngrams...   \n",
    "    print('feature file:', feature_file)\n",
    "    \n",
    "    feature_counts = []\n",
    "    with open(feature_file, 'w') as f:\n",
    "        # Write the column names for the csv file\n",
    "        fw = writer(f)\n",
    "        # Do this when combining the files.\n",
    "        #colnames = ['file_name'] + ['entropy'] + ['file_size'] \n",
    "        #fw.writerow(colnames)\n",
    "        \n",
    "        # Now iterate through the file list and extract the features from each file.\n",
    "        for idx, fname in enumerate(tfiles):\n",
    "            fasm = open(ext_drive + fname, 'rb')\n",
    "            filesize = os.path.getsize(ext_drive + fname)\n",
    "            in_bytes = fasm.read()\n",
    "            \n",
    "            # TODO: Do ngram extraction\n",
    "            # First do entropy calculations and filesize\n",
    "            # Convert the input array into a byte array to prevent type errors\n",
    "            # in entropy counter function.\n",
    "            in_bytes = bytearray(in_bytes)\n",
    "            #print(\"Type = {:s}\").format(type(in_bytes))\n",
    "            entropy = entropy_counter(in_bytes)\n",
    "            \n",
    "            count_vals = [entropy, filesize]\n",
    "            \n",
    "            feature_counts.append([fname[fname.find('_')+1:]] + count_vals)   \n",
    "            \n",
    "            fasm.close()\n",
    "            \n",
    "            # Print progress\n",
    "            if (idx+1) % 1000 == 0:\n",
    "                print(\"{:d} - {:d} of {:d} files processed.\".format(pid, idx + 1, ftot))\n",
    "                fw.writerows(feature_counts)\n",
    "                feature_counts = []\n",
    "                \n",
    "        # Write remaining files\n",
    "        if len(feature_counts) > 0:\n",
    "            fw.writerows(feature_counts)\n",
    "            feature_counts = []\n",
    "\n",
    "        print(\"Completed processing {:d} rows for feature file {:s}\".format(ftot,feature_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "# Now divide the train files into four groups for multiprocessing\n",
    "ext_drive = '/opt/vs/train/'\n",
    "tfiles = os.listdir(ext_drive)\n",
    "quart = len(tfiles)/4\n",
    "train1 = tfiles[:quart]\n",
    "train2 = tfiles[quart:(2*quart)]\n",
    "train3 = tfiles[(2*quart):(3*quart)]\n",
    "train4 = tfiles[(3*quart):]\n",
    "print(\"Files: {:d} - {:d} - {:d}\".format(len(tfiles), quart, (len(train1)+len(train2)+len(train3)+len(train4))))\n",
    "trains = [train1, train2, train3, train4]\n",
    "p = Pool(4)\n",
    "p.map(extract_binary_features, trains)\n",
    "combine_entropy_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files: 65536 - 16384 - 65536\n",
      "('Process id:', 4152)\n",
      "('feature file:', 'data/4153-entropy-features-bin.csv')\n",
      "('feature file:', 'data/4152-entropy-features-bin.csv')\n",
      "('Process id:', 4153)\n",
      "('Process id:', 4154)\n",
      "('feature file:', 'data/4154-entropy-features-bin.csv')\n",
      "('Process id:', 4155)\n",
      "('feature file:', 'data/4155-entropy-features-bin.csv')\n",
      "4153 - 1000 of 16384 files processed.\n",
      "4152 - 1000 of 16384 files processed.\n",
      "4155 - 1000 of 16384 files processed.\n",
      "4154 - 1000 of 16384 files processed.\n",
      "4152 - 2000 of 16384 files processed.\n",
      "4153 - 2000 of 16384 files processed.\n",
      "4155 - 2000 of 16384 files processed.\n",
      "4154 - 2000 of 16384 files processed.\n",
      "4152 - 3000 of 16384 files processed.\n",
      "4155 - 3000 of 16384 files processed.\n",
      "4153 - 3000 of 16384 files processed.\n",
      "4154 - 3000 of 16384 files processed.\n",
      "4152 - 4000 of 16384 files processed.\n",
      "4155 - 4000 of 16384 files processed.\n",
      "4153 - 4000 of 16384 files processed.\n",
      "4154 - 4000 of 16384 files processed.\n",
      "4155 - 5000 of 16384 files processed.\n",
      "4152 - 5000 of 16384 files processed.\n",
      "4153 - 5000 of 16384 files processed.\n",
      "4154 - 5000 of 16384 files processed.\n",
      "4155 - 6000 of 16384 files processed.\n",
      "4152 - 6000 of 16384 files processed.\n",
      "4153 - 6000 of 16384 files processed.\n",
      "4154 - 6000 of 16384 files processed.\n",
      "4155 - 7000 of 16384 files processed.\n",
      "4152 - 7000 of 16384 files processed.\n",
      "4153 - 7000 of 16384 files processed.\n",
      "4154 - 7000 of 16384 files processed.\n",
      "4155 - 8000 of 16384 files processed.\n",
      "4153 - 8000 of 16384 files processed.\n",
      "4152 - 8000 of 16384 files processed.\n",
      "4154 - 8000 of 16384 files processed.\n",
      "4155 - 9000 of 16384 files processed.\n",
      "4153 - 9000 of 16384 files processed.\n",
      "4152 - 9000 of 16384 files processed.\n",
      "4154 - 9000 of 16384 files processed.\n",
      "4155 - 10000 of 16384 files processed.\n",
      "4154 - 10000 of 16384 files processed.\n",
      "4153 - 10000 of 16384 files processed.\n",
      "4152 - 10000 of 16384 files processed.\n",
      "4155 - 11000 of 16384 files processed.\n",
      "4154 - 11000 of 16384 files processed.\n",
      "4153 - 11000 of 16384 files processed.\n",
      "4152 - 11000 of 16384 files processed.\n",
      "4155 - 12000 of 16384 files processed.\n",
      "4154 - 12000 of 16384 files processed.\n",
      "4153 - 12000 of 16384 files processed.\n",
      "4152 - 12000 of 16384 files processed.\n",
      "4155 - 13000 of 16384 files processed.\n",
      "4154 - 13000 of 16384 files processed.\n",
      "4153 - 13000 of 16384 files processed.\n",
      "4155 - 14000 of 16384 files processed.\n",
      "4152 - 13000 of 16384 files processed.\n",
      "4154 - 14000 of 16384 files processed.\n",
      "4153 - 14000 of 16384 files processed.\n",
      "4152 - 14000 of 16384 files processed.\n",
      "4155 - 15000 of 16384 files processed.\n",
      "4154 - 15000 of 16384 files processed.\n",
      "4153 - 15000 of 16384 files processed.\n",
      "4155 - 16000 of 16384 files processed.\n",
      "4152 - 15000 of 16384 files processed.\n",
      "4154 - 16000 of 16384 files processed.\n",
      "Completed processing 16384 rows for feature file data/4155-entropy-features-bin.csv\n",
      "Completed processing 16384 rows for feature file data/4154-entropy-features-bin.csv\n",
      "4153 - 16000 of 16384 files processed.\n",
      "4152 - 16000 of 16384 files processed.\n",
      "Completed processing 16384 rows for feature file data/4153-entropy-features-bin.csv\n",
      "Completed processing 16384 rows for feature file data/4152-entropy-features-bin.csv\n",
      "Completed combine of 65536 entropy features.\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "# Now divide the train files into four groups for multiprocessing\n",
    "ext_drive = '/opt/vs/train2/'\n",
    "tfiles = os.listdir(ext_drive)\n",
    "quart = len(tfiles)/4\n",
    "train1 = tfiles[:quart]\n",
    "train2 = tfiles[quart:(2*quart)]\n",
    "train3 = tfiles[(2*quart):(3*quart)]\n",
    "train4 = tfiles[(3*quart):]\n",
    "print(\"Files: {:d} - {:d} - {:d}\".format(len(tfiles), quart, (len(train1)+len(train2)+len(train3)+len(train4))))\n",
    "trains = [train1, train2, train3, train4]\n",
    "p = Pool(4)\n",
    "p.map(extract_binary_features, trains)\n",
    "combine_entropy_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>entropy</th>\n",
       "      <th>file_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0    </th>\n",
       "      <td> 00002e640cafb741bea9a48eaee27d6f</td>\n",
       "      <td> 0.992174</td>\n",
       "      <td>  208860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1    </th>\n",
       "      <td> 000118d12cbf9ad6103e8b914a6e1ac3</td>\n",
       "      <td> 0.834382</td>\n",
       "      <td>  201600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2    </th>\n",
       "      <td> 0001776237ac37a69fcef93c1bac0988</td>\n",
       "      <td> 0.966021</td>\n",
       "      <td>  682192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65536</th>\n",
       "      <td> 00027c21667d9119a454df8cef2dc1c7</td>\n",
       "      <td> 0.666599</td>\n",
       "      <td>   18390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65537</th>\n",
       "      <td> 0003887ab64b8ae19ffa988638decac2</td>\n",
       "      <td> 0.903260</td>\n",
       "      <td> 1134320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3    </th>\n",
       "      <td> 000403e4e488356b7535cc613fbeb80b</td>\n",
       "      <td> 0.773787</td>\n",
       "      <td>  199168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65538</th>\n",
       "      <td> 0004376a62e22f6ad359467eb742b8ff</td>\n",
       "      <td> 0.803515</td>\n",
       "      <td>  149720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4    </th>\n",
       "      <td> 0004c8b2a0f4680a5694d74199b40ea2</td>\n",
       "      <td> 0.985592</td>\n",
       "      <td> 1165440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5    </th>\n",
       "      <td> 000595d8b586915c12053104cf845097</td>\n",
       "      <td> 0.841920</td>\n",
       "      <td>  264240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65539</th>\n",
       "      <td> 000634f03457d088c71dbffb897b1315</td>\n",
       "      <td> 0.957584</td>\n",
       "      <td> 1725502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65540</th>\n",
       "      <td> 00072ed24314e91b63b425b3dc572f50</td>\n",
       "      <td> 0.486112</td>\n",
       "      <td>  328093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65541</th>\n",
       "      <td> 00092d369958b67557da8661cc9093bc</td>\n",
       "      <td> 0.845657</td>\n",
       "      <td>  522936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6    </th>\n",
       "      <td> 00093d5fa5cb7ce77f6eaf39962daa12</td>\n",
       "      <td> 0.803481</td>\n",
       "      <td>  742064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7    </th>\n",
       "      <td> 00099926d51b44c6f8c93a48c2567891</td>\n",
       "      <td> 0.997032</td>\n",
       "      <td>  725288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65542</th>\n",
       "      <td> 0009a64f786fa29bfa6423278cc74f02</td>\n",
       "      <td> 0.996663</td>\n",
       "      <td>  671280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8    </th>\n",
       "      <td> 000a2db4762dc06628a086c9e117f884</td>\n",
       "      <td> 0.535436</td>\n",
       "      <td>   61551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65543</th>\n",
       "      <td> 000ac11fa7587b2316470b154254a219</td>\n",
       "      <td> 0.997824</td>\n",
       "      <td> 1874471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9    </th>\n",
       "      <td> 000ae2c63ba69fc93dfc395b40bfe03a</td>\n",
       "      <td> 0.899481</td>\n",
       "      <td>  487386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65544</th>\n",
       "      <td> 000ae90736a51c47543dcc6d8a735362</td>\n",
       "      <td> 0.863887</td>\n",
       "      <td>  260144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65545</th>\n",
       "      <td> 000b41258d624ef2d6e430822d0c0c8f</td>\n",
       "      <td> 0.992772</td>\n",
       "      <td>  590824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              file_name   entropy  file_size\n",
       "0      00002e640cafb741bea9a48eaee27d6f  0.992174     208860\n",
       "1      000118d12cbf9ad6103e8b914a6e1ac3  0.834382     201600\n",
       "2      0001776237ac37a69fcef93c1bac0988  0.966021     682192\n",
       "65536  00027c21667d9119a454df8cef2dc1c7  0.666599      18390\n",
       "65537  0003887ab64b8ae19ffa988638decac2  0.903260    1134320\n",
       "3      000403e4e488356b7535cc613fbeb80b  0.773787     199168\n",
       "65538  0004376a62e22f6ad359467eb742b8ff  0.803515     149720\n",
       "4      0004c8b2a0f4680a5694d74199b40ea2  0.985592    1165440\n",
       "5      000595d8b586915c12053104cf845097  0.841920     264240\n",
       "65539  000634f03457d088c71dbffb897b1315  0.957584    1725502\n",
       "65540  00072ed24314e91b63b425b3dc572f50  0.486112     328093\n",
       "65541  00092d369958b67557da8661cc9093bc  0.845657     522936\n",
       "6      00093d5fa5cb7ce77f6eaf39962daa12  0.803481     742064\n",
       "7      00099926d51b44c6f8c93a48c2567891  0.997032     725288\n",
       "65542  0009a64f786fa29bfa6423278cc74f02  0.996663     671280\n",
       "8      000a2db4762dc06628a086c9e117f884  0.535436      61551\n",
       "65543  000ac11fa7587b2316470b154254a219  0.997824    1874471\n",
       "9      000ae2c63ba69fc93dfc395b40bfe03a  0.899481     487386\n",
       "65544  000ae90736a51c47543dcc6d8a735362  0.863887     260144\n",
       "65545  000b41258d624ef2d6e430822d0c0c8f  0.992772     590824\n",
       "\n",
       "[20 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropys = pd.read_csv('data/sorted-entropy-features.csv')\n",
    "sorted_entropys = entropys.sort('file_name')\n",
    "sorted_entropys.to_csv('data/sorted-entropy-features-vs251-252.csv', index=False)\n",
    "sorted_entropys.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Entropy and File Size of Unpacked Binaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate ASM and Header Features\n",
    "    - PE Header Features from objdump header summaries.\n",
    "    - ASM Features from IDA Pro assembly files.\n",
    "    \n",
    "    - Script: feature_extraction.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = ['Virtual','Offset','loc','Import','Imports','var','Forwarder','UINT','LONG'\n",
    "            ,'BOOL','WORD','BYTES','large','short','dd','db','dw','XREF','ptr','DATA'\n",
    "            ,'FUNCTION','extrn','byte','word','dword','char','DWORD','stdcall','arg'\n",
    "            ,'locret','asc','align','WinMain','unk','cookie','off','nullsub','DllEntryPoint'\n",
    "            ,'System32','dll','CHUNK','BASS','HMENU','DLL','LPWSTR','void','HRESULT','HDC'\n",
    "            ,'LRESULT','HANDLE','HWND','LPSTR','int','HLOCAL','FARPROC','ATOM','HMODULE'\n",
    "            ,'WPARAM','HGLOBAL','entry','rva','COLLAPSED','config','exe','Software'\n",
    "            ,'CurrentVersion','__imp_','INT_PTR','UINT_PTR','---Seperator','PCCTL_CONTEXT'\n",
    "            ,'__IMPORT_','INTERNET_STATUS_CALLBACK','.rdata:','.data:','.text:','case'\n",
    "            ,'installdir','market','microsoft','policies','proc','scrollwindow','search'\n",
    "            ,'trap','visualc','___security_cookie','assume','callvirtualalloc','exportedentry'\n",
    "            ,'hardware','hkey_current_user','hkey_local_machine','sp-analysisfailed','unableto']\n",
    "\n",
    "known_sections = ['.text', '.data', '.bss', '.rdata', '.edata', '.idata', '.rsrc', '.tls', '.reloc']\n",
    "\n",
    "registers = ['edx','esi','es','fs','ds','ss','gs','cs','ah','al',\n",
    "                 'ax','bh','bl','bx','ch','cl','cx','dh','dl','dx',\n",
    "                 'eax','ebp','ebx','ecx','edi','esp']\n",
    "\n",
    "opcodes = ['add','al','bt','call','cdq','cld','cli','cmc','cmp','const','cwd','daa','db'\n",
    "                ,'dd','dec','dw','endp','ends','faddp','fchs','fdiv','fdivp','fdivr','fild'\n",
    "                ,'fistp','fld','fstcw','fstcwimul','fstp','fword','fxch','imul','in','inc'\n",
    "                ,'ins','int','jb','je','jg','jge','jl','jmp','jnb','jno','jnz','jo','jz'\n",
    "                ,'lea','loope','mov','movzx','mul','near','neg','not','or','out','outs'\n",
    "                ,'pop','popf','proc','push','pushf','rcl','rcr','rdtsc','rep','ret','retn'\n",
    "                ,'rol','ror','sal','sar','sbb','scas','setb','setle','setnle','setnz'\n",
    "                ,'setz','shl','shld','shr','sidt','stc','std','sti','stos','sub','test'\n",
    "                ,'wait','xchg','xor']\n",
    "\n",
    "\n",
    "def count_asm_symbols(asm_code):\n",
    "    symbols = [0]*7\n",
    "    for row in asm_code:\n",
    "        if '*' in row:\n",
    "            symbols[0] += 1\n",
    "        if '-' in row:\n",
    "            symbols[1] += 1\n",
    "        if '+' in row:\n",
    "            symbols[2] += 1\n",
    "        if '[' in row:\n",
    "            symbols[3] += 1\n",
    "        if ']' in row:\n",
    "            symbols[4] += 1\n",
    "        if '@' in row:\n",
    "            symbols[5] += 1\n",
    "        if '?' in row:\n",
    "            symbols[6] += 1\n",
    "\n",
    "    return symbols\n",
    "\n",
    "\n",
    "def count_asm_registers(asm_code):\n",
    "    registers_values = [0]*len(registers)\n",
    "    for row in asm_code:\n",
    "        parts = row.replace(',',' ').replace('+',' ').replace('*',' ').replace('[',' ').replace(']',' ') \\\n",
    "                    .replace('-',' ').split()\n",
    "        for register in registers:\n",
    "            registers_values[registers.index(register)] += parts.count(register)\n",
    "    return registers_values\n",
    "\n",
    "\n",
    "def count_asm_opcodes(asm_code):\n",
    "    opcodes_values = [0]*len(opcodes)\n",
    "    for row in asm_code:\n",
    "        parts = row.split()\n",
    "\n",
    "        for opcode in opcodes:\n",
    "            if opcode in parts:\n",
    "                opcodes_values[opcodes.index(opcode)] += 1\n",
    "                break\n",
    "    return opcodes_values\n",
    "\n",
    "\n",
    "def count_asm_APIs(asm_code, apis):\n",
    "    apis_values = [0]*len(apis)\n",
    "    for row in asm_code:\n",
    "        for i in range(len(apis)):\n",
    "            if apis[i] in row:\n",
    "                apis_values[i] += 1\n",
    "                break\n",
    "    return apis_values\n",
    "\n",
    "\n",
    "def count_asm_misc(asm_code):\n",
    "    keywords_values = [0]*len(keywords)\n",
    "    for row in asm_code:\n",
    "        for i in range(len(keywords)):\n",
    "            if keywords[i] in row:\n",
    "                keywords_values[i] += 1\n",
    "                break\n",
    "    return keywords_values\n",
    "\n",
    "\n",
    "# Extract features from test/training asm files, file list is passed in as a parameter\n",
    "\n",
    "def extract_asm_features(tfiles):\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    print('Process id:', pid)\n",
    "    feature_file = 'data/' + str(pid) + '-malware-features-asm.csv' # Windows API, symbols, registers, opcodes, etc...   \n",
    "    print('feature file:', feature_file)\n",
    "\n",
    "    fapi = open(\"data/APIs.txt\")\n",
    "    defined_apis = fapi.readlines()\n",
    "    defined_apis = defined_apis[0].split(',')\n",
    "\n",
    "    asm_files = [i for i in tfiles if '.asm' in i]\n",
    "    ftot = len(asm_files)\n",
    "    \n",
    "    feature_counts = []\n",
    "    with open(feature_file, 'w') as f:\n",
    "        # write the csv header\n",
    "        fw = writer(f)\n",
    "        colnames = ['filename'] + registers + opcodes + defined_apis + keywords\n",
    "        fw.writerow(colnames)\n",
    "        \n",
    "        for idx, fname in enumerate(asm_files):\n",
    "            fasm = open(ext_drive + fname, 'r')\n",
    "            content = fasm.readlines()\n",
    "            \n",
    "            reg_vals = count_asm_registers(content)\n",
    "            opc_vals = count_asm_opcodes(content)\n",
    "            api_vals = count_asm_APIs(content, defined_apis)\n",
    "            #sec_vals = count_asm_sections(content)\n",
    "            mis_vals = count_asm_misc(content)\n",
    "            count_vals = reg_vals + opc_vals + api_vals + mis_vals\n",
    "            \n",
    "            feature_counts.append([fname[:fname.find('.asm')]] + count_vals)   \n",
    "            \n",
    "            # Writing rows after every 10 files processed\n",
    "            if (idx+1) % 10 == 0:\n",
    "              print(pid, idx + 1, 'of', ftot, 'files processed.')\n",
    "              fw.writerows(feature_counts)\n",
    "              feature_counts = []\n",
    "                \n",
    "        # Writing remaining files\n",
    "        if len(feature_counts) > 0:\n",
    "            fw.writerows(feature_counts)\n",
    "            feature_counts = []\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Need to clean up and sort these function names for ASM feature extraction.\n",
    "fip = open('data/all-function-column-names-multiline.csv')\n",
    "function_names = fip.readlines()\n",
    "fip.close()\n",
    "function_names[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\"\"\"\"\\n',\n",
       " '\"\"\"%s\"\\n',\n",
       " '\"\"\"%x\"\\n',\n",
       " '\"\"\"&i)\"\\n',\n",
       " '\"\"\"&len)\"\\n',\n",
       " '\"\"\")\"\\n',\n",
       " '\"\"\"AbortDoc\"\\n',\n",
       " '\"\"\"CoInitialize\"\\n',\n",
       " '\"\"\"GetRecentFWCounters\"\\n',\n",
       " '\"\"\"InternetConnectA(\"\\n',\n",
       " '\"\"\"IsInterface\"\\n',\n",
       " '\"\"\"NetConnection.connect\"\\n',\n",
       " '\"\"\"NetStream.createStream\"\\n',\n",
       " '\"\"\"NetStream.play\"\\n',\n",
       " '\"\"\"SubTabPosition\"\\n',\n",
       " '\"\"\"__w\"\\n',\n",
       " '\"\"\"add\"\\n',\n",
       " '\"\"\"analysis\"\\n',\n",
       " '\"\"\"argument\"\\n',\n",
       " '\"\"\"bool)\"\\n',\n",
       " '\"\"\"cancelled\"\\n',\n",
       " '\"\"\"decrypt\"\\n',\n",
       " '\"\"\"default\"\\n',\n",
       " '\"\"\"division\"\\n',\n",
       " '\"\"\"doesn\"\\n',\n",
       " '\"\"\"draw_corner_width\"\\n',\n",
       " '\"\"\"eith\"\\n',\n",
       " '\"\"\"ex-viewer\"\\n',\n",
       " '\"\"\"fa\"\\n',\n",
       " '\"\"\"fail\"\\n',\n",
       " '\"\"\"failed\"\\n',\n",
       " '\"\"\"failed.\"\\n',\n",
       " '\"\"\"filtered\"\\n',\n",
       " '\"\"\"gettrafficmeter_t&)\"\\n',\n",
       " '\"\"\"handle_reply_for_bootstrap_r\"\\n',\n",
       " '\"\"\"ignored\"\\n',\n",
       " '\"\"\"int)\"\\n',\n",
       " '\"\"\"l)\"\\n',\n",
       " '\"\"\"m_pIFWCntrs\"\\n',\n",
       " '\"\"\"m_pInfcCache\"\\n',\n",
       " '\"\"\"maxchars\"\\n',\n",
       " '\"\"\"object\"\\n',\n",
       " '\"\"\"pIFWCntr\"\\n',\n",
       " '\"\"\"pIFWCntrs2\"\\n',\n",
       " '\"\"\"settrafficmeter_t&)\"\\n',\n",
       " '\"\"\"size\"\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function_names.sort()\n",
    "function_names[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fop = open('data/sorted-function-names-multiline.txt','w')\n",
    "fop.writelines(function_names)\n",
    "fop.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
