{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Construction of Call Graphs From PE ASM Files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import os\n",
    "from csv import writer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import scipy.misc\n",
    "import array\n",
    "import time as tm\n",
    "import re\n",
    "import subprocess as sub\n",
    "import graph as gra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "call_opcodes = ['call', 'int']\n",
    "call_blocks = ['sub_', 'main', 'start']\n",
    "\n",
    "def construct_call_graph(lines, log_file):\n",
    "    vertex = '.program_entry_point' # this is the root node, corresponds to the program original entry point not C main().\n",
    "    vertex_count = 1\n",
    "    edge_count = 0\n",
    "    cfgraph = gra.Graph()\n",
    "    cfgraph.add_vertex(vertex)\n",
    "    \n",
    "    for row in lines:\n",
    "        row = row.rstrip('\\r\\n')  # get rid of newlines they are annoying.\n",
    "        if row.startswith(';'):\n",
    "            continue\n",
    "        if ';' in row:\n",
    "            row = row.split(';')[0] # get rid of comments they are annoying.\n",
    "            #print(row)\n",
    "      \n",
    "        if 'call' in row or ' int ' in row:\n",
    "            out_line = \"->  {:s}\".format(row)\n",
    "            log_file.write(out_line + \"\\n\")\n",
    "            \n",
    "        if row.startswith(\"sub_\"):\n",
    "            out_line = \"Vertex:  {:s}\".format(row)\n",
    "            log_file.write(out_line + \"\\n\")            \n",
    "            \n",
    "        # get rid of all these things they are annoying.\n",
    "        row = row.replace('short','').replace('ds:',' ')\n",
    "        row = row.replace('dword','').replace('near','')\n",
    "        row = row.replace('ptr','').replace(',',' ') #.replace(':',' ').replace(',',' ')\n",
    "        row = row.replace('@','').replace('?','')\n",
    "        parts = row.split() # tokenize code line\n",
    "        \n",
    "        if (len(parts) < 2): # this is just a comment line\n",
    "            continue\n",
    "        \n",
    "        if ('endp' in parts): # ignore subroutine end labels\n",
    "            continue\n",
    "        \n",
    "        # check for subroutines and block labels\n",
    "        # block and subroutine labels are always after the .text HHHHHHHH relative address\n",
    "        for block in call_blocks:\n",
    "            token = parts[0]  \n",
    "            idx = token.find(block)\n",
    "            if ((idx == 0) or ('proc' in parts)):\n",
    "                # add new vertex to the graph, we are now in a new subroutine\n",
    "                vertex = token\n",
    "                cfgraph.add_vertex(vertex)\n",
    "                vertex_count += 1\n",
    "                \n",
    "                out_line = \"Vertex: {:d}  {:s}\".format(vertex_count, vertex)\n",
    "                log_file.write(out_line + \"\\n\")\n",
    "                # print(out_line)\n",
    "            \n",
    "                break\n",
    "\n",
    "        # now check for edge opcode    \n",
    "        for opcode in call_opcodes: # check the line for a new edge\n",
    "            if opcode in parts:\n",
    "                # Extract desination address/function name/interrupt number as the directed edge.\n",
    "                idx = parts.index(opcode)\n",
    "                edge_count += 1\n",
    "                if ((idx + 1) < len(parts)): # in a few ASM files there is no operand, disassembly error?\n",
    "                    next_vertex = parts[idx + 1]\n",
    "                else:\n",
    "                    next_vertex = \"none\"\n",
    "                cfgraph.add_edge(vertex, next_vertex)\n",
    "                # print(\"Edge: \" + vertex + \" \" + parts[idx] + \" \" + edge)\n",
    "                break\n",
    "\n",
    "    # print(\"Vertex Count: {:d}\".format(vertex_count))\n",
    "    \n",
    "    return cfgraph\n",
    "\n",
    "\n",
    "def extract_call_graphs(multi_params):\n",
    "    asm_files = multi_params.file_list\n",
    "    ftot = len(asm_files)\n",
    "    ext_drive = multi_params.ext_drive\n",
    "    \n",
    "    pid = os.getpid()\n",
    "    feature_file = 'data/' + str(pid) + \"-\" + multi_params.feature_file \n",
    "    log_file_name = 'data/' + str(pid) + \"-\" + multi_params.feature_file + \".log\"\n",
    "    log_file = open(log_file_name, 'w')\n",
    "    \n",
    "    print('Process ID: {:d} Graph Feature file: {:s}'.format(pid, feature_file))\n",
    "    \n",
    "    graph_lines = []\n",
    "    graph_features = []\n",
    "    graph_file = open('data/' + str(pid) + \"-\" + multi_params.graph_file, 'w') # write as a graphviz DOT format file\n",
    "    \n",
    "    with open(feature_file, 'w') as f:\n",
    "        # write the column names for the csv file\n",
    "        fw = writer(f)\n",
    "        #colnames = ['filename','vertex_count','edge_count','delta_max','density','diameter']\n",
    "        #colnames = ['file_name','vertex_count','edge_count','delta_max','density']\n",
    "        #fw.writerow(colnames) put in combine_feature_files\n",
    "        \n",
    "        # Now iterate through the file list and extract the call graph from each file.\n",
    "        for idx, fname in enumerate(asm_files):\n",
    "            fasm = open(ext_drive + fname, 'r') #, errors='ignore')\n",
    "            lines = fasm.readlines()\n",
    "            fasm.close()\n",
    "            \n",
    "            call_graph = construct_call_graph(lines, log_file)\n",
    "            cgvc = call_graph.n_vertices()\n",
    "            cgec = call_graph.n_edges()\n",
    "            cgdm = call_graph.delta_max()\n",
    "            cgde = call_graph.density()\n",
    "            \n",
    "            # cdia = call_graph.diameter() this is constantly problematic !!!\n",
    "            \n",
    "            fname_parts = fname.split('_') # Truncate the file name to the hash value.\n",
    "            trunc_name = fname_parts[1]\n",
    "            trunc_name = trunc_name[:trunc_name.find('.pe.asm')]\n",
    "            \n",
    "            graph_features.append([trunc_name] + [cgvc, cgec, cgdm, cgde])\n",
    "            call_graph.set_graph_name(trunc_name)\n",
    "            #graph_lines.append(call_graph.to_str('multinoleaf')) \n",
    "            graph_lines.append(call_graph.to_str('graphviz'))\n",
    "            \n",
    "            del(call_graph) # for some reason new graphs get appended to the previous graphs if not deleted???\n",
    "            \n",
    "            # Print progress\n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(pid, idx + 1, 'of', ftot, 'files processed.')\n",
    "                fw.writerows(graph_features)\n",
    "                graph_file.writelines(graph_lines)\n",
    "                graph_features = []\n",
    "                graph_lines = []\n",
    "                \n",
    "        # Write remaining files\n",
    "        if len(graph_lines) > 0:\n",
    "            fw.writerows(graph_features)\n",
    "            graph_file.writelines(graph_lines)\n",
    "            graph_features = []\n",
    "            graph_lines = []\n",
    "\n",
    "    graph_file.close()\n",
    "    log_file.close()\n",
    "    \n",
    "    print('Process ID: {:d} finished.'.format(pid))\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def combine_feature_files(out_file):\n",
    "    # Function to combine the newly generated call graph feature files into one file:\n",
    "    # 1. list data directory\n",
    "    # 2. For each file in file list that matches (\\d\\d\\d\\d-call-graph-features.csv)\n",
    "    # 3. Trim the filenames if necessary (should remove VirusShare_  prefix).\n",
    "    # 4. Concatenate the unsorted asm feature files.\n",
    "    # 5. Sort and write to data/sorted-call-graph-features.csv\n",
    "    \n",
    "    fop = open('data/sorted-' + out_file, 'w')\n",
    "    colnames = 'file_name,vertex_count,edge_count,delta_max,density'\n",
    "    fop.write(colnames)\n",
    "    \n",
    "    print(\"Column names: {:s}\".format(colnames))\n",
    "    \n",
    "    p1 = re.compile('\\d{3,5}-' + out_file) # This is the PID prefix for each file.\n",
    "    file_list = os.listdir('data/')\n",
    "    counter = 0\n",
    "    \n",
    "    for file_name in file_list:\n",
    "        if p1.match(file_name):\n",
    "            fip = open('data/' + file_name, 'r')\n",
    "            in_lines = fip.readlines()\n",
    "            fop.writelines(in_lines)\n",
    "            counter += len(in_lines)\n",
    "            fip.close()\n",
    "            \n",
    "    print('Completed combine of {:d} call graph features.'.format(counter))  \n",
    "    \n",
    "    fop.close()\n",
    "    \n",
    "    cgs = pd.read_csv('data/' + out_file)\n",
    "    # DataFrame.sort() is deprecated, but this is an old version of pandas, does not have sort_values().\n",
    "    sorted_cgs = cgs.sort('file_name')\n",
    "    sorted_cgs.to_csv('data/sorted-' + out_file, index=False)\n",
    "    sorted_cgs.head(20)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def combine_graph_files(out_file):\n",
    "    # Function to combine the newly generated call graph files into one file:\n",
    "    # 1. list data directory\n",
    "    # 2. For each file in file list that matches (\\d\\d\\d\\d-call-graphs.gv)\n",
    "    # 3. Trim the filenames if necessary (should remove VirusShare_  prefix).\n",
    "    # 4. Concatenate the unsorted asm feature files.\n",
    "    # 5. Sort and write to data/sorted-call-graphs.gv\n",
    "    \n",
    "    fop = open('data/' + out_file, 'w')\n",
    "    \n",
    "    p1 = re.compile('\\d{3,5}-' + out_file) # This is the PID prefix for each file.\n",
    "    file_list = os.listdir('data/')\n",
    "    counter = 0\n",
    "    \n",
    "    for file_name in file_list:\n",
    "        if p1.match(file_name):\n",
    "            fip = open('data/' + file_name, 'r')\n",
    "            in_lines = fip.readlines()\n",
    "            fop.writelines(in_lines)\n",
    "            counter += len(in_lines)\n",
    "            fip.close()\n",
    "            \n",
    "    print('Completed combine of {:d} call graph lines.'.format(counter))  \n",
    "    \n",
    "    fop.close()\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "class Multi_Params(object):\n",
    "    def __init__(self, featurefile=\"\", graphfile=\"\", extdrive=\"\", filelist=[]):\n",
    "        self.feature_file = featurefile\n",
    "        self.graph_file = graphfile\n",
    "        self.ext_drive = extdrive\n",
    "        self.file_list = filelist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process ID: 3646 Graph Feature file: data/3646-sorted-pe-call-graph-features-apt.csv\n",
      "(3646, 10, 'of', 271, 'files processed.')\n",
      "(3646, 20, 'of', 271, 'files processed.')\n",
      "(3646, 30, 'of', 271, 'files processed.')\n",
      "(3646, 40, 'of', 271, 'files processed.')\n",
      "(3646, 50, 'of', 271, 'files processed.')\n",
      "(3646, 60, 'of', 271, 'files processed.')\n",
      "(3646, 70, 'of', 271, 'files processed.')\n",
      "(3646, 80, 'of', 271, 'files processed.')\n",
      "(3646, 90, 'of', 271, 'files processed.')\n",
      "(3646, 100, 'of', 271, 'files processed.')\n",
      "(3646, 110, 'of', 271, 'files processed.')\n",
      "(3646, 120, 'of', 271, 'files processed.')\n",
      "(3646, 130, 'of', 271, 'files processed.')\n",
      "(3646, 140, 'of', 271, 'files processed.')\n",
      "(3646, 150, 'of', 271, 'files processed.')\n",
      "(3646, 160, 'of', 271, 'files processed.')\n",
      "(3646, 170, 'of', 271, 'files processed.')\n",
      "(3646, 180, 'of', 271, 'files processed.')\n",
      "(3646, 190, 'of', 271, 'files processed.')\n",
      "(3646, 200, 'of', 271, 'files processed.')\n",
      "(3646, 210, 'of', 271, 'files processed.')\n",
      "(3646, 220, 'of', 271, 'files processed.')\n",
      "(3646, 230, 'of', 271, 'files processed.')\n",
      "(3646, 240, 'of', 271, 'files processed.')\n",
      "(3646, 250, 'of', 271, 'files processed.')\n",
      "(3646, 260, 'of', 271, 'files processed.')\n",
      "(3646, 270, 'of', 271, 'files processed.')\n",
      "Process ID: 3646 finished.\n"
     ]
    }
   ],
   "source": [
    "feature_file = 'sorted-pe-call-graph-features-apt.csv'\n",
    "graph_file = 'pe-call-graphs-apt.gv'\n",
    "ext_drive = '/opt/vs/aptasm/'\n",
    "file_ext = '-apt'\n",
    "\n",
    "file_list = os.listdir(ext_drive)\n",
    "tfiles = [i for i in file_list if '.pe.asm' in i]\n",
    "\n",
    "quart = len(tfiles)/4\n",
    "train1 = tfiles[:quart]\n",
    "train2 = tfiles[quart:(2*quart)]\n",
    "train3 = tfiles[(2*quart):(3*quart)]\n",
    "train4 = tfiles[(3*quart):]\n",
    "\n",
    "#mp1 = Multi_Params(feature_file, graph_file, ext_drive, train1)\n",
    "#mp2 = Multi_Params(feature_file, graph_file, ext_drive, train2)\n",
    "#mp3 = Multi_Params(feature_file, graph_file, ext_drive, train3)\n",
    "#mp4 = Multi_Params(feature_file, graph_file, ext_drive, train4)\n",
    "\n",
    "# Single process test.\n",
    "mp1 = Multi_Params(feature_file, graph_file, ext_drive, tfiles)\n",
    "extract_call_graphs(mp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
